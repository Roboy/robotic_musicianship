{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import pypianoroll as ppr\n",
    "import time\n",
    "import music21\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from utils.utilsPreprocessing import *\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "#torch.set_printoptions(threshold=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##########HYPERPARAMS#####################\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.999\n",
    "batch_size = 1 #CHANGE THIS VERWRIRUNG\n",
    "seq_length = 8\n",
    "log_interval = 10 #Log/show loss per batch\n",
    "input_size=100\n",
    "##########################################\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../YamahaPianoCompetition2002NoTranspose.npz')\n",
    "#midiDatasetTrain = data['train']\n",
    "midiDatasetTest = data['test']\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#midiDatasetTrain = torch.from_numpy(midiDatasetTrain)\n",
    "#trainLoader = torch.utils.data.DataLoader(midiDatasetTrain, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "midiDatasetTest = torch.from_numpy(midiDatasetTest)\n",
    "testLoader = torch.utils.data.DataLoader(midiDatasetTest, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.LSTM_Predicter import LSTM\n",
    "from utils.VAE_ReLU import VAE\n",
    "from loadModel import loadModel\n",
    "\n",
    "#for gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#load models\n",
    "lstmModel = LSTM()\n",
    "autoencoderModel = VAE()\n",
    "\n",
    "#load weights\n",
    "lstmModel = loadModel(lstmModel, '../models/LSTM_NEW_YamahaPianoComp2002_100Epochs.model')\n",
    "autoencoderModel = loadModel(autoencoderModel, \n",
    "                             '../models/YamahaPC2002_VAE_Reconstruct_NoTW_10Epochs_ReLU.model',\n",
    "                            dataParallelModel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new sample by feeding 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "playSeq = 0\n",
    "lstmModel.batch_size=1\n",
    "\n",
    "if(lstmModel.train()):\n",
    "    lstmModel.eval()\n",
    "if(autoencoderModel.train()):\n",
    "    autoencoderModel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pathToSampleSeq in glob.glob('../WikifoniaDatabase/test/*.mid'):\n",
    "        sampleNp1 = getSlicedPianorollMatrixNp(pathToSampleSeq)\n",
    "        sampleNp1 = deleteZeroMatrices(sampleNp1)\n",
    "        sample = np.expand_dims(sampleNp1[0,:,36:-32],axis=0)\n",
    "        #print(sample.shape)\n",
    "        for i, sampleNp in enumerate(sampleNp1[playSeq:playSeq+(seq_length-1)]):\n",
    "            #print(sampleNp.shape)\n",
    "            if(np.any(sampleNp)):\n",
    "                sampleNp = sampleNp[:,36:-32]\n",
    "                sampleNp = np.expand_dims(sampleNp,axis=0)\n",
    "                sample = np.concatenate((sample,sampleNp),axis=0)\n",
    "        samplePlay = sample[0,:,:]\n",
    "        for s in sample[1:]:\n",
    "            samplePlay = np.concatenate((samplePlay,s),axis=0)\n",
    "        samplePlay = addCuttedOctaves(samplePlay)\n",
    "        #print(samplePlay.shape)\n",
    "        \n",
    "        #####PREPARE SAMPLE for input\n",
    "        sample = torch.from_numpy(sample).float().to(device)\n",
    "        sample = torch.unsqueeze(sample,1)\n",
    "\n",
    "        #####MODEL##############\n",
    "        embed, _ = autoencoderModel.encoder(sample)\n",
    "        print(embed.size())\n",
    "        embed = embed.unsqueeze(0).double()\n",
    "        embed, lstmOut = lstmModel(embed, future=0)\n",
    "        lstmOut = lstmOut.float()\n",
    "        print(lstmOut.size())\n",
    "        #recon = autoencoderModel.decoder(embed.float())\n",
    "        #print(recon.size())\n",
    "        pred = autoencoderModel.decoder(lstmOut)\n",
    "        print(pred.size())\n",
    "        ########################\n",
    "\n",
    "        predict = pred.squeeze(0).squeeze(0).cpu().numpy()\n",
    "        print(predict.shape)\n",
    "        #print(predict.shape)\n",
    "\n",
    "        #NORMALIZE PREDICTIONS\n",
    "        #reconstruction /= np.abs(np.max(reconstruction))\n",
    "        prediction /= np.abs(np.max(prediction))\n",
    "        #print(prediction)\n",
    "\n",
    "        #CHECK MIDI ACTIVATIONS IN PREDICTION TO INCLUDE RESTS\n",
    "        #reconstruction[reconstruction < 0.3] = 0\n",
    "        prediction[prediction < 0.3] = 0\n",
    "        #print(prediction)\n",
    "\n",
    "        samplePlay = debinarizeMidi(samplePlay, prediction=False)\n",
    "        samplePlay = addCuttedOctaves(samplePlay)\n",
    "        #reconstruction = debinarizeMidi(reconstruction, prediction=True)\n",
    "        #reconstruction = addCuttedOctaves(reconstruction)\n",
    "        prediction = debinarizeMidi(prediction, prediction=True)\n",
    "        prediction = addCuttedOctaves(prediction)\n",
    "        print(\"INPUT\")\n",
    "        print(samplePlay.shape)\n",
    "        pianorollMatrixToTempMidi(samplePlay, show=True,showPlayer=True,autoplay=False)\n",
    "        #print(\"RECONSTRUCTION\")\n",
    "        #pianorollMatrixToTempMidi(reconstruction, show=True,\n",
    "        #                            showPlayer=True,autoplay=True, prediction=True)\n",
    "        print(\"PREDICTION\")\n",
    "        pianorollMatrixToTempMidi(prediction, prediction=True, \n",
    "                                  show=True,showPlayer=True,autoplay=True)        \n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
