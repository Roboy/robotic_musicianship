{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import numpy as np\n",
    "import glob\n",
    "import pypianoroll as ppr\n",
    "import pretty_midi\n",
    "import time\n",
    "import music21\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from utils.utilsPreprocessing import *\n",
    "\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "#torch.set_printoptions(threshold=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#HYPERPARAMS\n",
    "##################################\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "batch_size = 200\n",
    "log_interval = 1000 #Log/show loss per batch\n",
    "embedding_size = 100\n",
    "beat_resolution = 24\n",
    "seq_length = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 2688316 sequences\n",
      "The training set contains 2419484 sequences\n",
      "The validation set contains 268832 sequences\n"
     ]
    }
   ],
   "source": [
    "#create dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "path_to_files = \"/media/EXTHD/niciData/DATASETS/AllSequences/\"\n",
    "\n",
    "dataset = createDatasetAE((path_to_files + \"*.npy\"), \n",
    "                           beat_res = beat_resolution, \n",
    "                           seq_length = seq_length,\n",
    "                           binarize=True)\n",
    "print(\"Dataset contains {} sequences\".format(len(dataset)))\n",
    "    \n",
    "train_size = int(np.floor(0.9 * len(dataset)))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "print(\"The training set contains {} sequences\".format(len(train_dataset)))\n",
    "print(\"The validation set contains {} sequences\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load dataset from npz\n",
    "\"\"\"\n",
    "data = np.load('../WikifoniaPartlyNoTranspose.npz')\n",
    "midiDatasetTrain = data['train']\n",
    "midiDatasetTest = data['test']\n",
    "data.close()\n",
    "\n",
    "print(midiDatasetTrain.shape)\n",
    "print(midiDatasetTest.shape)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(midiDatasetTrain, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(midiDatasetTest, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fullPitch = 128; reducedPitch = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, embedding_size=100):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        ###ENCODER###\n",
    "        self.encode1 = nn.Sequential(\n",
    "            nn.Conv2d(1,100,(16,5),stride=(16,5),padding=0),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(100,200,(2,1),stride=(2,1),padding=0),\n",
    "            nn.BatchNorm2d(200),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(200,400,(2,2),stride=(1,2),padding=0),\n",
    "            nn.BatchNorm2d(400),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(400,800,(2,2),stride=(2,2),padding=0),\n",
    "            nn.BatchNorm2d(800),\n",
    "            nn.ELU()\n",
    "        )\n",
    "            \n",
    "        self.encode2 = nn.Sequential(\n",
    "            nn.Linear(2400,800),\n",
    "            nn.BatchNorm1d(800),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(800,400),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.ELU()\n",
    "            #nn.Linear(400,100),\n",
    "            #nn.BatchNorm1d(100),\n",
    "            #nn.ELU()\n",
    "        )\n",
    "        self.encode31 = nn.Sequential(\n",
    "            nn.Linear(400,self.embedding_size),\n",
    "            nn.BatchNorm1d(self.embedding_size),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.encode32 = nn.Sequential(\n",
    "            nn.Linear(400,self.embedding_size),\n",
    "            nn.BatchNorm1d(self.embedding_size),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        ###DECODER###\n",
    "        self.decode1 = nn.Sequential(\n",
    "            nn.Linear(self.embedding_size,400),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(400,800),\n",
    "            nn.BatchNorm1d(800),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(800,2400),\n",
    "            nn.BatchNorm1d(2400),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.decode2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(800,400,(2,2),stride=(2,2),padding=0),\n",
    "            nn.BatchNorm2d(400),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(400,200,(2,2),stride=(1,2),padding=0),\n",
    "            nn.BatchNorm2d(200),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(200,100,(2,1),stride=(2,1),padding=0),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(100,1,(16,5),stride=(16,5),padding=0),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ELU()\n",
    "        )\n",
    "    \n",
    "    def encoder(self, hEnc):\n",
    "        #print(\"ENOCDER\")\n",
    "        hEnc = self.encode1(hEnc)\n",
    "        hEnc = torch.squeeze(hEnc,3).view(-1,800*3)\n",
    "        hEnc = self.encode2(hEnc)\n",
    "        hEnc1 = self.encode31(hEnc)\n",
    "        hEnc2 = self.encode32(hEnc)\n",
    "        return hEnc1, hEnc2\n",
    "\n",
    "    def decoder(self, z):\n",
    "        #print(\"DECODER\")\n",
    "        hDec = self.decode1(z)\n",
    "        hDec = hDec.view(hDec.size(0),800,-1).unsqueeze(2)\n",
    "        hDec = self.decode2(hDec)\n",
    "        return hDec\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.2*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            #print(\"no change\")\n",
    "            return mu\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "    \n",
    "model = VAE(embedding_size=embedding_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-8) \n",
    "    #beta for disentanglement\n",
    "    beta = 1e0 \n",
    "\n",
    "    \"\"\"\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "    \"\"\"\n",
    "    batch = x.size(0)\n",
    "    cosSim = torch.sum(cos(x.view(batch,-1),recon_x.view(batch,-1)))\n",
    "    cosSim = batch-cosSim\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KLD /= mu.size(0) * mu.size(1)\n",
    "    #print(\"loss values: cossim=\",cosSim,\"KLD=\",KLD)\n",
    "    return cosSim + (beta * KLD), cosSim, KLD\n",
    "        \n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    trainLoss = 0\n",
    "    cos_sims = 0\n",
    "    klds = 0\n",
    "    loss_divider = len(train_loader.dataset)-(len(train_loader.dataset)%batch_size)\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        ###DENOISING AUTOENCODER\n",
    "        #data = data.float().to(device)\n",
    "        #noise = torch.bernoulli((torch.rand_like(data))).to(device)\n",
    "        #print(noise[0])\n",
    "        #noisyData = data+noise\n",
    "        #reconBatch, mu = model(noisyData)\n",
    "        \n",
    "        ###NORMAL AUTOENCODER\n",
    "        data = data.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconBatch, mu, logvar = model(data)\n",
    "\n",
    "        loss, cos_sim, kld = loss_function(reconBatch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        trainLoss += loss.item()\n",
    "        cos_sims += cos_sim\n",
    "        klds += kld\n",
    "        optimizer.step()\n",
    "        if(batch_idx % log_interval == 0):\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average Loss: {:.4f}'.format(\n",
    "          epoch, trainLoss / loss_divider))\n",
    "    \n",
    "    return trainLoss / loss_divider, cos_sims / loss_divider, klds / loss_divider\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    cos_sim = 0\n",
    "    kld = 0\n",
    "    loss_divider = len(valid_loader.dataset)-(len(valid_loader.dataset)%batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            ###DENOISING AUTOENCODER\n",
    "            #data = data.float().to(device)\n",
    "            #noise = torch.bernoulli((torch.rand_like(data))).to(device)\n",
    "            #noisyData = data+noise\n",
    "            #reconBatch, mu = model(noisyData)\n",
    "            \n",
    "            ###NORMAL AUTOENCODER\n",
    "            data = data.float().to(device)\n",
    "            reconBatch, mu, logvar = model(data)\n",
    "            \n",
    "            temp_valid_loss, cos_sim_temp, kld_temp = loss_function(reconBatch, data, mu, logvar)\n",
    "            valid_loss += temp_valid_loss.item()\n",
    "            cos_sim += cos_sim_temp.item()\n",
    "            kld += kld_temp.item()\n",
    "            #if(i==10):\n",
    "            #    break\n",
    "    valid_loss /= loss_divider\n",
    "\n",
    "    print('====> Test set loss: {:.4f}'.format(valid_loss))\n",
    "    return valid_loss, cos_sim, kld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from loadModel import loadModel\n",
    "#pathToModel = '../models/YamahaPC2002_VAE_Reconstruct_NoTW_20Epochs.model'\n",
    "\n",
    "#model = loadModel(model,pathToModel, dataParallelModel=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2419484 (0%)]\tLoss: 0.973706\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "save_path = '/media/EXTHD/niciData/models/VAE_YamahaPC2002_{}LR_{}SeqLength'.format(str(learning_rate), seq_length)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "cos_sims_train = []\n",
    "klds_train = []\n",
    "cos_sims_test = []\n",
    "klds_test = []\n",
    "best_valid_loss = 999.\n",
    "for epoch in range(1, epochs + 1):\n",
    "    #training with plots\n",
    "    train_loss, cos_sim_train, kld_train = train(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    cos_sims_train.append(cos_sim_train)\n",
    "    klds_train.append(kld_train)\n",
    "    \n",
    "    #validate with plots\n",
    "    current_valid_loss, cos_sim_test, kld_test = validate(epoch)\n",
    "    valid_losses.append(current_valid_loss)\n",
    "    cos_sims_test.append(cos_sim_test)\n",
    "    klds_test.append(kld_test)\n",
    "    \n",
    "    #save if model better than before\n",
    "    if(current_valid_loss < best_valid_loss):\n",
    "        best_valid_loss = current_valid_loss\n",
    "        torch.save(model.state_dict(),(save_path + '.pth'))\n",
    "        \n",
    "    #plot current results\n",
    "    plt.plot(train_losses, color='red', label='Train Loss')\n",
    "    plt.plot(valid_losses, color='orange', label='Test Loss')\n",
    "    plt.plot(cos_sims_train, color='blue', label='Cosine Similarity Train')\n",
    "    plt.plot(klds_train, color='black', label='KL Divergence Train')\n",
    "    plt.plot(cos_sims_test, color='green', label='Cosine Similarity Test')\n",
    "    plt.plot(klds_test, color='cyan', label='KL Divergence Test')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    print('')\n",
    "        \n",
    "\n",
    "plt.plot(train_losses, color='red', label='Train Loss')\n",
    "plt.plot(valid_losses, color='orange', label='Test Loss')\n",
    "plt.plot(cos_sims_train, color='blue', label='Cosine Similarity Train')\n",
    "plt.plot(klds_train, color='black', label='KL Divergence Train')\n",
    "plt.plot(cos_sims_test, color='green', label='Cosine Similarity Test')\n",
    "plt.plot(klds_test, color='cyan', label='KL Divergence Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(save_path + '.png')\n",
    "plt.show()\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model,'/media/EXTHD/niciData/models/YamahaPC2002_DenoisingCVAE_Reconstruct_NoTW_5Epochs.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.set_printoptions(precision=4, suppress=True, threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if(model.train()):\n",
    "    model.eval()\n",
    "    \n",
    "###PLAY WHOLE SONG IN BARS\n",
    "with torch.no_grad():\n",
    "    sampleNp1 = getSlicedPianorollMatrixNp(\"../WikifoniaDatabase/test/Hermann-Lohr,-D.-Eardley-Wilmot---Little-Grey-Home-In-The-West.mid\")\n",
    "    sampleNp1 = deleteZeroMatrices(sampleNp1)\n",
    "    \n",
    "    for i,sampleNp in enumerate(sampleNp1[:8]):\n",
    "        sampleNp = sampleNp[:,36:-32]\n",
    "        sample = torch.from_numpy(sampleNp).float()\n",
    "        embed, logvar = model.encoder(sample.reshape(1,1,seq_length,reducedPitch).to(device))\n",
    "        ###RECONSTRUCTION#########\n",
    "        pred = model.decoder(embed)\n",
    "        ##########################\n",
    "        ###RANDOM RECONSTRUCTION##\n",
    "        std = torch.exp(0.2*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        randomRecon = model.decoder(eps.mul(std).add_(embed))\n",
    "        ##########################\n",
    "\n",
    "        reconstruction = pred.squeeze(0).squeeze(0).cpu().numpy()\n",
    "        randomRecon = randomRecon.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "        #NORMALIZE PREDICTIONS\n",
    "        reconstruction /= np.abs(np.max(reconstruction))\n",
    "        randomRecon /= np.abs(np.max(randomRecon))\n",
    "\n",
    "        #CHECK MIDI ACTIVATIONS IN PREDICTION TO INCLUDE RESTS\n",
    "        reconstruction[reconstruction < 0.2] = 0\n",
    "        randomRecon[randomRecon < 0.3] = 0\n",
    "\n",
    "        samplePlay = debinarizeMidi(sampleNp, prediction=False)\n",
    "        samplePlay = addCuttedOctaves(samplePlay)\n",
    "        reconstruction = debinarizeMidi(reconstruction, prediction=True)\n",
    "        reconstruction = addCuttedOctaves(reconstruction)\n",
    "        randomRecon = debinarizeMidi(randomRecon, prediction=True)\n",
    "        randomRecon = addCuttedOctaves(randomRecon)\n",
    "        if(i==0):\n",
    "            sampleOut = samplePlay\n",
    "            reconOut = reconstruction\n",
    "            randomReconOut = randomRecon\n",
    "        else:\n",
    "            sampleOut = np.concatenate((sampleOut,samplePlay), axis=0)\n",
    "            reconOut = np.concatenate((reconOut,reconstruction), axis=0)\n",
    "            randomReconOut = np.concatenate((randomReconOut, randomRecon),axis=0)\n",
    "\n",
    "\n",
    "    print(\"INPUT\")\n",
    "    pianorollMatrixToTempMidi(sampleOut, show=True,showPlayer=True,autoplay=False,\n",
    "                             path='../temp/inputTemp.mid')\n",
    "    print(\"RECONSTRUCTION\")\n",
    "    pianorollMatrixToTempMidi(reconOut, show=True,showPlayer=True,autoplay=False,\n",
    "                             path='../temp/reconTemp.mid')\n",
    "    #print(\"Reconstruction with Noise\")\n",
    "    #pianorollMatrixToTempMidi(randomReconOut, show=True,showPlayer=True,autoplay=False,\n",
    "    #                         path='../temp/noiseReconTemp.mid')  \n",
    "    print(\"\\n\\n\")\n",
    "            \n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morph from one sequence to another in latent space\n",
    "\n",
    "Take 2 embedded sequences and slowly morph it to the other one based on this formula: $c_\\alpha=(1-\\alpha)*z_1 + \\alpha*z_2$ , where $z_1$ corresponds to the embedding of sample 1 and $z_2$ to sample 2.\n",
    "(Source of formula: https://arxiv.org/pdf/1803.05428.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    #get 2 different unseeen sequences and choose random sequence\n",
    "    sample1 = getSlicedPianorollMatrixNp('../WikifoniaDatabase/train/1952,-Jerry-Lieber-&-Mike-Stoller---Kansas-City.mid')\n",
    "    sample1 = sample1[6,:,36:-32]\n",
    "    sample2 = getSlicedPianorollMatrixNp('../WikifoniaDatabase/test/Hoagy-Carmichael---Blue-Orchids.mid')\n",
    "    sample2 = sample2[7,:,36:-32]\n",
    "    print('sample1 shape:', sample1.shape)\n",
    "    print('sample2 shape: ',sample2.shape)\n",
    "\n",
    "    #prepare for input\n",
    "    sample1 = torch.from_numpy(sample1.reshape(1,1,96,60)).float().to(device)\n",
    "    sample2 = torch.from_numpy(sample2.reshape(1,1,96,60)).float().to(device)\n",
    "    \n",
    "    #run through encoder\n",
    "    embed1, _ = model.encoder(sample1)\n",
    "    embed2, _ = model.encoder(sample2)\n",
    "    \n",
    "    for a in range(0,11):\n",
    "        alpha = a/10\n",
    "        print(\"alpha is \",alpha)\n",
    "        c = (1-alpha) * embed1 + alpha * embed2\n",
    "        \n",
    "        #decode current\n",
    "        recon = model.decoder(c)\n",
    "        recon = recon.squeeze(0).squeeze(0).cpu().numpy()\n",
    "        recon /= np.max(np.abs(recon))\n",
    "        recon[recon < 0.2] = 0\n",
    "        recon = debinarizeMidi(recon, prediction=True)\n",
    "        recon = addCuttedOctaves(recon)\n",
    "        pianorollMatrixToTempMidi(recon, show=True,showPlayer=False,autoplay=True,\n",
    "                             path='../temp/temp.mid')\n",
    "        \n",
    "\"\"\"       \n",
    "print('')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
