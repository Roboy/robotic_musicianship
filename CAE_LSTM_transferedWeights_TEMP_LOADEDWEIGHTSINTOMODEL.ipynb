{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import glob\n",
    "import pypianoroll as ppr\n",
    "import time\n",
    "import music21\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from utils.utilsPreprocessing import *\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "#torch.set_printoptions(threshold=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#HYPERPARAMS\n",
    "##################################\n",
    "epochs = 1\n",
    "learning_rate = 1e-3\n",
    "batch_size= 98\n",
    "log_interval = 1  #Log/show loss per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MIDI files from npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (39782, 1, 96, 60)\n",
      "Test set: (9691, 1, 96, 60)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/Volumes/EXT/DATASETS/YamahaPianoCompetition2002NoTranspose.npz')\n",
    "\n",
    "midiDatasetTrain = data['train']\n",
    "midiDatasetTest = data['test']\n",
    "\n",
    "data.close()\n",
    "\n",
    "\"\"\"\n",
    "print(\"Training set: ({}, {}, {}, {})\".format(midiDatasetTrain.size()[0],\n",
    "                                                midiDatasetTrain.size()[1],\n",
    "                                                midiDatasetTrain.size()[2],\n",
    "                                                midiDatasetTrain.size()[3]))\n",
    "print(\"Test set: ({}, {}, {}, {})\".format(midiDatasetTest.size()[0],\n",
    "                                                midiDatasetTest.size()[1],\n",
    "                                                midiDatasetTest.size()[2],\n",
    "                                                midiDatasetTest.size()[3]))\n",
    "\"\"\"\n",
    "\n",
    "print(\"Training set: {}\".format(midiDatasetTrain.shape))\n",
    "print(\"Test set: {}\".format(midiDatasetTest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fullPitch = 128\n",
    "_, _, length, reducedPitch = midiDatasetTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 16, 5])\n"
     ]
    }
   ],
   "source": [
    "#MODEL FOR TRANSFER LEARNING\n",
    "from utils.CDVAE import CDVAE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weightModel = CDVAE().to(device)\n",
    "\n",
    "print(weightModel.encode1[0].weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "midiDatasetTrain = torch.from_numpy(midiDatasetTrain)\n",
    "trainLoader = torch.utils.data.DataLoader(midiDatasetTrain, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "midiDatasetTest = torch.from_numpy(midiDatasetTest)\n",
    "testLoader = torch.utils.data.DataLoader(midiDatasetTest, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CDVAE(nn.Module):\n",
    "    def __init__(self, batch_size=7, tie_weights=True):\n",
    "        super(CDVAE, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        ###LSTM###\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=400,\n",
    "                            num_layers=3, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(400,100)\n",
    "        self.eluFC = nn.ELU()\n",
    "    \n",
    "    def encoder(self, hEnc):\n",
    "        #print(\"ENOCDER\")\n",
    "        i=0\n",
    "        for j in range(4):\n",
    "            hEnc = F.conv2d(hEnc,\n",
    "                            weight=weightModel.encode1[i].weight,\n",
    "                            bias=weightModel.encode1[i].bias, \n",
    "                            stride=weightModel.encode1[i].stride,\n",
    "                            padding=0)\n",
    "            i+=1\n",
    "            hEnc = F.batch_norm(hEnc, \n",
    "                                running_mean=weightModel.encode1[i].running_mean, \n",
    "                                running_var=weightModel.encode1[i].running_var, \n",
    "                                weight=weightModel.encode1[i].weight,\n",
    "                                bias=weightModel.encode1[i].bias,\n",
    "                                training=self.training)\n",
    "            hEnc = F.elu(hEnc)            \n",
    "            i+=2\n",
    "        hEnc = torch.squeeze(hEnc,3).view(-1,800*3)\n",
    "        i=0\n",
    "        for j in range(3):\n",
    "            hEnc = F.linear(hEnc,weightModel.encode2[i].weight,weightModel.encode2[i].bias)\n",
    "            i+=1\n",
    "            hEnc = F.batch_norm(hEnc, \n",
    "                                running_mean=weightModel.encode2[i].running_mean, \n",
    "                                running_var=weightModel.encode2[i].running_var, \n",
    "                                weight=weightModel.encode2[i].weight,\n",
    "                                bias=weightModel.encode2[i].bias,\n",
    "                                training=self.training)\n",
    "            hEnc = F.elu(hEnc)        \n",
    "            i+=2\n",
    "        return hEnc\n",
    "\n",
    "    def decoder(self, hDec):\n",
    "        #print(\"DECODER\")\n",
    "        i=0\n",
    "        for j in range(3):\n",
    "            hDec = F.linear(hDec, weightModel.decode1[i].weight, weightModel.decode1[i].bias)\n",
    "            i+=1\n",
    "            hDec = F.batch_norm(hDec, \n",
    "                                running_mean=weightModel.decode1[i].running_mean, \n",
    "                                running_var=weightModel.decode1[i].running_var, \n",
    "                                weight=weightModel.decode1[i].weight,\n",
    "                                bias=weightModel.decode1[i].bias,\n",
    "                                training=self.training)        \n",
    "            hDec = F.elu(hDec)\n",
    "            i+=2\n",
    "        hDec = hDec.view(hDec.size()[0],800,-1).unsqueeze(2)\n",
    "        i=0\n",
    "        for j in range(4):\n",
    "            hDec = F.conv_transpose2d(hDec, \n",
    "                                      weight=weightModel.decode2[i].weight, \n",
    "                                      bias=weightModel.decode2[i].bias, \n",
    "                                      stride=weightModel.decode2[i].stride, \n",
    "                                      padding=0)\n",
    "            i+=1\n",
    "            hDec = F.batch_norm(hDec, \n",
    "                                running_mean=weightModel.decode2[i].running_mean, \n",
    "                                running_var=weightModel.decode2[i].running_var, \n",
    "                                weight=weightModel.decode2[i].weight,\n",
    "                                bias=weightModel.decode2[i].bias,\n",
    "                                training=self.training) \n",
    "            hDec = F.elu(hDec)\n",
    "            i+=2\n",
    "        return hDec\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.encoder(x)\n",
    "        \n",
    "        ####MOVE TO HIDDEN_INIT\n",
    "        h_t = torch.zeros(3,int(embed.size()[0]/7),400).to(device)\n",
    "        c_t = torch.zeros(3,int(embed.size()[0]/7),400).to(device)        \n",
    "        ###HIDDEN INIT END\n",
    "        \n",
    "        #IF FOR TESTING UNKNOWN SEQUENCES\n",
    "        if(embed.size()[0]>7):\n",
    "            embedTemp = torch.chunk(embed, int(self.batch_size/7),dim=0)\n",
    "            embed7s = embedTemp[0].unsqueeze(0)\n",
    "            for emb in embedTemp[1:]:\n",
    "                #print(\"inloop\");print(emb.unsqueeze(1).size())\n",
    "                embed7s = torch.cat((embed7s, emb.unsqueeze(0)),dim=0)\n",
    "                #print(\"afterconcat\");print(embed7s.size())\n",
    "        else:\n",
    "            embed7s = embed.unsqueeze(0)\n",
    "            \n",
    "        lstmOut, (h_t, c_t) = self.lstm(embed7s,(h_t, c_t))\n",
    "        lstmOut = self.fc(lstmOut)\n",
    "        lstmOut = self.eluFC(lstmOut)\n",
    "        #print(lstmOut.size())\n",
    "        \n",
    "        recon_lstm = lstmOut[0,:,:]\n",
    "        #print(recon_lstm.size())\n",
    "        for output in lstmOut[1:]:\n",
    "            recon_lstm = torch.cat((recon_lstm,output),dim=0)\n",
    "        #print(recon_lstm.size())\n",
    "        return embed, lstmOut, self.decoder(recon_lstm)\n",
    "\n",
    "    \n",
    "\n",
    "model = CDVAE(batch_size=batch_size).to(device)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=learning_rate, weight_decay=1e-1)\n",
    "\n",
    "def loss_function(x, recon_lstm, embed, lstmOut):\n",
    "    \n",
    "    cosLSTM = nn.CosineSimilarity(dim=0, eps=1e-8) \n",
    "    batch_sizeMin1 = x.size()[0]-1\n",
    "    \n",
    "    #BATCHSIZE 7\n",
    "    ###WRONG SINCE LOSS CHANGED TO COMPARE EVERY\n",
    "    ###PREDICTED SEQUENCE WITH THE NEXT\n",
    "    if(embed.size()[0]==7):\n",
    "        #print(\"batchsize = 7 ?\")\n",
    "        cosSimLSTM = cosLSTM(lstmOut.squeeze(1)[-1], embedNext[0])\n",
    "     \n",
    "    #BATCHSIZE > 7\n",
    "    else:\n",
    "        cosSimLSTM = 0\n",
    "        k=1\n",
    "        for batchOut in lstmOut:\n",
    "            for out in batchOut:\n",
    "                #print(k)\n",
    "                cosSimLSTM += cosLSTM(out,embed[k])\n",
    "                k+=1\n",
    "                if(k==embed.size()[0]-1):\n",
    "                    break\n",
    "        ###TAKE 1st sequence of next batch and compare it to last LSTM output           \n",
    "        #cosSimLSTM += cosLSTM(lstmOut[-1,-1,:],embedNext[0])\n",
    "        cosSimLSTM = batch_sizeMin1-cosSimLSTM\n",
    "        \n",
    "        ###RECONSTRUCTION LOS ON PREDICTIONS\n",
    "        #cos = nn.CosineSimilarity(dim=1, eps=1e-8) \n",
    "        #cosSim = torch.sum(cos(x[1:].view(batch_sizeMin1,-1),\n",
    "        #                       recon_lstm[:-1].view(batch_sizeMin1,-1)))\n",
    "        #cosSim = batch_sizeMin1-cosSim\n",
    "        ###RECONSTRUCTION LOSS END\n",
    "        \n",
    "        totalLoss = cosSimLSTM# + cosSim\n",
    "        \n",
    "    return totalLoss\n",
    "        \n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    trainLoss = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(trainLoader):\n",
    "        #print(batch_idx)\n",
    "        data = data.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embedding, lstmOut, reconPrediction = model(data)\n",
    "        #nextBatch = next(iter(trainLoader)).float().to(device)\n",
    "        #with torch.no_grad():\n",
    "        #    embeddingNext, _, _ = model(nextBatch)\n",
    "        #print(nextBatch.size())\n",
    "        loss = loss_function(data, reconPrediction, embedding, lstmOut)\n",
    "        loss.backward()\n",
    "        trainLoss += loss.item()\n",
    "        optimizer.step()\n",
    "        if(batch_idx % log_interval == 0):\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainLoader.dataset),\n",
    "                100. * batch_idx / len(trainLoader),\n",
    "                loss.item() / len(data)))\n",
    "        #if(batch_idx==1):\n",
    "        #   break\n",
    "    print('====> Epoch: {} Average Loss: {:.4f}'.format(\n",
    "          epoch, trainLoss / (len(trainLoader.dataset)-batch_idx)))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    testLoss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testLoader):\n",
    "            data = data.float().to(device)\n",
    "            embedding, lstmOut, reconPrediction = model(data)\n",
    "            #nextBatch = next(iter(testLoader)).float().to(device)\n",
    "            #embeddingNext, _, _= model(nextBatch)\n",
    "            testLoss += loss_function(data, reconPrediction, embedding, lstmOut).item()\n",
    "            \n",
    "            #if(i==1):\n",
    "            #    break\n",
    "    testLoss /= (len(testLoader.dataset)-i)\n",
    "\n",
    "    print('====> Test set Loss: {:.4f}'.format(testLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#LOAD MODEL\n",
    "pathToModel = '../models/WikifoniaNoTranpose_10Epochs_LSTM_noTW_dropout50.model'\n",
    "\n",
    "try:\n",
    "    #LOAD TRAINED MODEL INTO GPU\n",
    "    if(torch.cuda.is_available()):\n",
    "        model = torch.load(pathToModel)\n",
    "        \n",
    "    #LOAD MODEL TRAINED ON GPU INTO CPU\n",
    "    else:\n",
    "        model = torch.load(pathToModel, map_location=lambda storage, loc: storage)\n",
    "    print(\"\\n--------model restored--------\\n\")\n",
    "except:\n",
    "    print(\"\\n--------no saved model found--------\\n\")\n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/39782 (0%)]\tLoss: 0.986482\n",
      "Train Epoch: 1 [98/39782 (0%)]\tLoss: 0.857721\n",
      "Train Epoch: 1 [196/39782 (0%)]\tLoss: 0.867190\n",
      "Train Epoch: 1 [294/39782 (1%)]\tLoss: 0.854384\n",
      "Train Epoch: 1 [392/39782 (1%)]\tLoss: 0.845054\n",
      "Train Epoch: 1 [490/39782 (1%)]\tLoss: 0.848214\n",
      "Train Epoch: 1 [588/39782 (1%)]\tLoss: 0.844994\n",
      "Train Epoch: 1 [686/39782 (2%)]\tLoss: 0.837566\n",
      "Train Epoch: 1 [784/39782 (2%)]\tLoss: 0.841161\n",
      "Train Epoch: 1 [882/39782 (2%)]\tLoss: 0.840951\n",
      "Train Epoch: 1 [980/39782 (2%)]\tLoss: 0.844185\n",
      "Train Epoch: 1 [1078/39782 (3%)]\tLoss: 0.850387\n",
      "Train Epoch: 1 [1176/39782 (3%)]\tLoss: 0.841184\n",
      "Train Epoch: 1 [1274/39782 (3%)]\tLoss: 0.840449\n",
      "Train Epoch: 1 [1372/39782 (3%)]\tLoss: 0.837594\n",
      "Train Epoch: 1 [1470/39782 (4%)]\tLoss: 0.840962\n",
      "Train Epoch: 1 [1568/39782 (4%)]\tLoss: 0.846301\n",
      "Train Epoch: 1 [1666/39782 (4%)]\tLoss: 0.844698\n",
      "Train Epoch: 1 [1764/39782 (4%)]\tLoss: 0.846567\n",
      "Train Epoch: 1 [1862/39782 (5%)]\tLoss: 0.838281\n",
      "Train Epoch: 1 [1960/39782 (5%)]\tLoss: 0.852005\n",
      "Train Epoch: 1 [2058/39782 (5%)]\tLoss: 0.843078\n",
      "Train Epoch: 1 [2156/39782 (5%)]\tLoss: 0.841824\n",
      "Train Epoch: 1 [2254/39782 (6%)]\tLoss: 0.849881\n",
      "Train Epoch: 1 [2352/39782 (6%)]\tLoss: 0.845229\n",
      "Train Epoch: 1 [2450/39782 (6%)]\tLoss: 0.838366\n",
      "Train Epoch: 1 [2548/39782 (6%)]\tLoss: 0.840128\n",
      "Train Epoch: 1 [2646/39782 (7%)]\tLoss: 0.841275\n",
      "Train Epoch: 1 [2744/39782 (7%)]\tLoss: 0.840542\n",
      "Train Epoch: 1 [2842/39782 (7%)]\tLoss: 0.841612\n",
      "Train Epoch: 1 [2940/39782 (7%)]\tLoss: 0.842594\n",
      "Train Epoch: 1 [3038/39782 (8%)]\tLoss: 0.834511\n",
      "Train Epoch: 1 [3136/39782 (8%)]\tLoss: 0.842283\n",
      "Train Epoch: 1 [3234/39782 (8%)]\tLoss: 0.845842\n",
      "Train Epoch: 1 [3332/39782 (8%)]\tLoss: 0.842734\n",
      "Train Epoch: 1 [3430/39782 (9%)]\tLoss: 0.841542\n",
      "Train Epoch: 1 [3528/39782 (9%)]\tLoss: 0.840654\n",
      "Train Epoch: 1 [3626/39782 (9%)]\tLoss: 0.846255\n",
      "Train Epoch: 1 [3724/39782 (9%)]\tLoss: 0.844926\n",
      "Train Epoch: 1 [3822/39782 (10%)]\tLoss: 0.843082\n",
      "Train Epoch: 1 [3920/39782 (10%)]\tLoss: 0.838548\n",
      "Train Epoch: 1 [4018/39782 (10%)]\tLoss: 0.845734\n",
      "Train Epoch: 1 [4116/39782 (10%)]\tLoss: 0.844176\n",
      "Train Epoch: 1 [4214/39782 (11%)]\tLoss: 0.844381\n",
      "Train Epoch: 1 [4312/39782 (11%)]\tLoss: 0.837102\n",
      "Train Epoch: 1 [4410/39782 (11%)]\tLoss: 0.837243\n",
      "Train Epoch: 1 [4508/39782 (11%)]\tLoss: 0.836950\n",
      "Train Epoch: 1 [4606/39782 (12%)]\tLoss: 0.841618\n",
      "Train Epoch: 1 [4704/39782 (12%)]\tLoss: 0.843194\n",
      "Train Epoch: 1 [4802/39782 (12%)]\tLoss: 0.838885\n",
      "Train Epoch: 1 [4900/39782 (12%)]\tLoss: 0.845296\n",
      "Train Epoch: 1 [4998/39782 (13%)]\tLoss: 0.842400\n",
      "Train Epoch: 1 [5096/39782 (13%)]\tLoss: 0.844190\n",
      "Train Epoch: 1 [5194/39782 (13%)]\tLoss: 0.848353\n",
      "Train Epoch: 1 [5292/39782 (13%)]\tLoss: 0.839737\n",
      "Train Epoch: 1 [5390/39782 (14%)]\tLoss: 0.848491\n",
      "Train Epoch: 1 [5488/39782 (14%)]\tLoss: 0.843904\n",
      "Train Epoch: 1 [5586/39782 (14%)]\tLoss: 0.837387\n",
      "Train Epoch: 1 [5684/39782 (14%)]\tLoss: 0.840550\n",
      "Train Epoch: 1 [5782/39782 (15%)]\tLoss: 0.843516\n",
      "Train Epoch: 1 [5880/39782 (15%)]\tLoss: 0.846155\n",
      "Train Epoch: 1 [5978/39782 (15%)]\tLoss: 0.838127\n",
      "Train Epoch: 1 [6076/39782 (15%)]\tLoss: 0.840923\n",
      "Train Epoch: 1 [6174/39782 (16%)]\tLoss: 0.836639\n",
      "Train Epoch: 1 [6272/39782 (16%)]\tLoss: 0.847685\n",
      "Train Epoch: 1 [6370/39782 (16%)]\tLoss: 0.839522\n",
      "Train Epoch: 1 [6468/39782 (16%)]\tLoss: 0.842111\n",
      "Train Epoch: 1 [6566/39782 (17%)]\tLoss: 0.852156\n",
      "Train Epoch: 1 [6664/39782 (17%)]\tLoss: 0.845612\n",
      "Train Epoch: 1 [6762/39782 (17%)]\tLoss: 0.842425\n",
      "Train Epoch: 1 [6860/39782 (17%)]\tLoss: 0.842120\n",
      "Train Epoch: 1 [6958/39782 (18%)]\tLoss: 0.836399\n",
      "Train Epoch: 1 [7056/39782 (18%)]\tLoss: 0.843261\n",
      "Train Epoch: 1 [7154/39782 (18%)]\tLoss: 0.842993\n",
      "Train Epoch: 1 [7252/39782 (18%)]\tLoss: 0.839940\n",
      "Train Epoch: 1 [7350/39782 (19%)]\tLoss: 0.840465\n",
      "Train Epoch: 1 [7448/39782 (19%)]\tLoss: 0.843907\n",
      "Train Epoch: 1 [7546/39782 (19%)]\tLoss: 0.847785\n",
      "Train Epoch: 1 [7644/39782 (19%)]\tLoss: 0.847374\n",
      "Train Epoch: 1 [7742/39782 (20%)]\tLoss: 0.838111\n",
      "Train Epoch: 1 [7840/39782 (20%)]\tLoss: 0.843610\n",
      "Train Epoch: 1 [7938/39782 (20%)]\tLoss: 0.848658\n",
      "Train Epoch: 1 [8036/39782 (20%)]\tLoss: 0.844820\n",
      "Train Epoch: 1 [8134/39782 (20%)]\tLoss: 0.840720\n",
      "Train Epoch: 1 [8232/39782 (21%)]\tLoss: 0.836599\n",
      "Train Epoch: 1 [8330/39782 (21%)]\tLoss: 0.841141\n",
      "Train Epoch: 1 [8428/39782 (21%)]\tLoss: 0.839657\n",
      "Train Epoch: 1 [8526/39782 (21%)]\tLoss: 0.841555\n",
      "Train Epoch: 1 [8624/39782 (22%)]\tLoss: 0.841400\n",
      "Train Epoch: 1 [8722/39782 (22%)]\tLoss: 0.842430\n",
      "Train Epoch: 1 [8820/39782 (22%)]\tLoss: 0.842532\n",
      "Train Epoch: 1 [8918/39782 (22%)]\tLoss: 0.836944\n",
      "Train Epoch: 1 [9016/39782 (23%)]\tLoss: 0.843864\n",
      "Train Epoch: 1 [9114/39782 (23%)]\tLoss: 0.839968\n",
      "Train Epoch: 1 [9212/39782 (23%)]\tLoss: 0.847778\n",
      "Train Epoch: 1 [9310/39782 (23%)]\tLoss: 0.843237\n",
      "Train Epoch: 1 [9408/39782 (24%)]\tLoss: 0.843632\n",
      "Train Epoch: 1 [9506/39782 (24%)]\tLoss: 0.841182\n",
      "Train Epoch: 1 [9604/39782 (24%)]\tLoss: 0.846156\n",
      "Train Epoch: 1 [9702/39782 (24%)]\tLoss: 0.838546\n",
      "Train Epoch: 1 [9800/39782 (25%)]\tLoss: 0.844402\n",
      "Train Epoch: 1 [9898/39782 (25%)]\tLoss: 0.842337\n",
      "Train Epoch: 1 [9996/39782 (25%)]\tLoss: 0.844806\n",
      "Train Epoch: 1 [10094/39782 (25%)]\tLoss: 0.837182\n",
      "Train Epoch: 1 [10192/39782 (26%)]\tLoss: 0.841204\n",
      "Train Epoch: 1 [10290/39782 (26%)]\tLoss: 0.845355\n",
      "Train Epoch: 1 [10388/39782 (26%)]\tLoss: 0.847059\n",
      "Train Epoch: 1 [10486/39782 (26%)]\tLoss: 0.845839\n",
      "Train Epoch: 1 [10584/39782 (27%)]\tLoss: 0.844525\n",
      "Train Epoch: 1 [10682/39782 (27%)]\tLoss: 0.839463\n",
      "Train Epoch: 1 [10780/39782 (27%)]\tLoss: 0.843326\n",
      "Train Epoch: 1 [10878/39782 (27%)]\tLoss: 0.840879\n",
      "Train Epoch: 1 [10976/39782 (28%)]\tLoss: 0.840951\n",
      "Train Epoch: 1 [11074/39782 (28%)]\tLoss: 0.845091\n",
      "Train Epoch: 1 [11172/39782 (28%)]\tLoss: 0.848209\n",
      "Train Epoch: 1 [11270/39782 (28%)]\tLoss: 0.839731\n",
      "Train Epoch: 1 [11368/39782 (29%)]\tLoss: 0.841588\n",
      "Train Epoch: 1 [11466/39782 (29%)]\tLoss: 0.839700\n",
      "Train Epoch: 1 [11564/39782 (29%)]\tLoss: 0.841741\n",
      "Train Epoch: 1 [11662/39782 (29%)]\tLoss: 0.838709\n",
      "Train Epoch: 1 [11760/39782 (30%)]\tLoss: 0.839419\n",
      "Train Epoch: 1 [11858/39782 (30%)]\tLoss: 0.841466\n",
      "Train Epoch: 1 [11956/39782 (30%)]\tLoss: 0.844721\n",
      "Train Epoch: 1 [12054/39782 (30%)]\tLoss: 0.830924\n",
      "Train Epoch: 1 [12152/39782 (31%)]\tLoss: 0.842092\n",
      "Train Epoch: 1 [12250/39782 (31%)]\tLoss: 0.842612\n",
      "Train Epoch: 1 [12348/39782 (31%)]\tLoss: 0.838525\n",
      "Train Epoch: 1 [12446/39782 (31%)]\tLoss: 0.850066\n",
      "Train Epoch: 1 [12544/39782 (32%)]\tLoss: 0.839779\n",
      "Train Epoch: 1 [12642/39782 (32%)]\tLoss: 0.837623\n",
      "Train Epoch: 1 [12740/39782 (32%)]\tLoss: 0.840288\n",
      "Train Epoch: 1 [12838/39782 (32%)]\tLoss: 0.846421\n",
      "Train Epoch: 1 [12936/39782 (33%)]\tLoss: 0.839021\n",
      "Train Epoch: 1 [13034/39782 (33%)]\tLoss: 0.840974\n",
      "Train Epoch: 1 [13132/39782 (33%)]\tLoss: 0.846888\n",
      "Train Epoch: 1 [13230/39782 (33%)]\tLoss: 0.846778\n",
      "Train Epoch: 1 [13328/39782 (34%)]\tLoss: 0.845115\n",
      "Train Epoch: 1 [13426/39782 (34%)]\tLoss: 0.844615\n",
      "Train Epoch: 1 [13524/39782 (34%)]\tLoss: 0.842307\n",
      "Train Epoch: 1 [13622/39782 (34%)]\tLoss: 0.840371\n",
      "Train Epoch: 1 [13720/39782 (35%)]\tLoss: 0.841858\n",
      "Train Epoch: 1 [13818/39782 (35%)]\tLoss: 0.834952\n",
      "Train Epoch: 1 [13916/39782 (35%)]\tLoss: 0.838483\n",
      "Train Epoch: 1 [14014/39782 (35%)]\tLoss: 0.842429\n",
      "Train Epoch: 1 [14112/39782 (36%)]\tLoss: 0.847203\n",
      "Train Epoch: 1 [14210/39782 (36%)]\tLoss: 0.838275\n",
      "Train Epoch: 1 [14308/39782 (36%)]\tLoss: 0.843191\n",
      "Train Epoch: 1 [14406/39782 (36%)]\tLoss: 0.839220\n",
      "Train Epoch: 1 [14504/39782 (37%)]\tLoss: 0.842703\n",
      "Train Epoch: 1 [14602/39782 (37%)]\tLoss: 0.839456\n",
      "Train Epoch: 1 [14700/39782 (37%)]\tLoss: 0.842894\n",
      "Train Epoch: 1 [14798/39782 (37%)]\tLoss: 0.844162\n",
      "Train Epoch: 1 [14896/39782 (38%)]\tLoss: 0.844380\n",
      "Train Epoch: 1 [14994/39782 (38%)]\tLoss: 0.845910\n",
      "Train Epoch: 1 [15092/39782 (38%)]\tLoss: 0.841880\n",
      "Train Epoch: 1 [15190/39782 (38%)]\tLoss: 0.849480\n",
      "Train Epoch: 1 [15288/39782 (39%)]\tLoss: 0.842480\n",
      "Train Epoch: 1 [15386/39782 (39%)]\tLoss: 0.836291\n",
      "Train Epoch: 1 [15484/39782 (39%)]\tLoss: 0.838590\n",
      "Train Epoch: 1 [15582/39782 (39%)]\tLoss: 0.847062\n",
      "Train Epoch: 1 [15680/39782 (40%)]\tLoss: 0.844654\n",
      "Train Epoch: 1 [15778/39782 (40%)]\tLoss: 0.839842\n",
      "Train Epoch: 1 [15876/39782 (40%)]\tLoss: 0.840499\n",
      "Train Epoch: 1 [15974/39782 (40%)]\tLoss: 0.842113\n",
      "Train Epoch: 1 [16072/39782 (40%)]\tLoss: 0.839865\n",
      "Train Epoch: 1 [16170/39782 (41%)]\tLoss: 0.840851\n",
      "Train Epoch: 1 [16268/39782 (41%)]\tLoss: 0.841387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [16366/39782 (41%)]\tLoss: 0.844172\n",
      "Train Epoch: 1 [16464/39782 (41%)]\tLoss: 0.839802\n",
      "Train Epoch: 1 [16562/39782 (42%)]\tLoss: 0.839116\n",
      "Train Epoch: 1 [16660/39782 (42%)]\tLoss: 0.839340\n",
      "Train Epoch: 1 [16758/39782 (42%)]\tLoss: 0.838103\n",
      "Train Epoch: 1 [16856/39782 (42%)]\tLoss: 0.839341\n",
      "Train Epoch: 1 [16954/39782 (43%)]\tLoss: 0.842762\n",
      "Train Epoch: 1 [17052/39782 (43%)]\tLoss: 0.842432\n",
      "Train Epoch: 1 [17150/39782 (43%)]\tLoss: 0.842179\n",
      "Train Epoch: 1 [17248/39782 (43%)]\tLoss: 0.836517\n",
      "Train Epoch: 1 [17346/39782 (44%)]\tLoss: 0.844370\n",
      "Train Epoch: 1 [17444/39782 (44%)]\tLoss: 0.839724\n",
      "Train Epoch: 1 [17542/39782 (44%)]\tLoss: 0.838511\n",
      "Train Epoch: 1 [17640/39782 (44%)]\tLoss: 0.842193\n",
      "Train Epoch: 1 [17738/39782 (45%)]\tLoss: 0.840868\n",
      "Train Epoch: 1 [17836/39782 (45%)]\tLoss: 0.841013\n",
      "Train Epoch: 1 [17934/39782 (45%)]\tLoss: 0.844906\n",
      "Train Epoch: 1 [18032/39782 (45%)]\tLoss: 0.847945\n",
      "Train Epoch: 1 [18130/39782 (46%)]\tLoss: 0.846216\n",
      "Train Epoch: 1 [18228/39782 (46%)]\tLoss: 0.842626\n",
      "Train Epoch: 1 [18326/39782 (46%)]\tLoss: 0.840942\n",
      "Train Epoch: 1 [18424/39782 (46%)]\tLoss: 0.838640\n",
      "Train Epoch: 1 [18522/39782 (47%)]\tLoss: 0.841821\n",
      "Train Epoch: 1 [18620/39782 (47%)]\tLoss: 0.839182\n",
      "Train Epoch: 1 [18718/39782 (47%)]\tLoss: 0.839512\n",
      "Train Epoch: 1 [18816/39782 (47%)]\tLoss: 0.841447\n",
      "Train Epoch: 1 [18914/39782 (48%)]\tLoss: 0.837431\n",
      "Train Epoch: 1 [19012/39782 (48%)]\tLoss: 0.837386\n",
      "Train Epoch: 1 [19110/39782 (48%)]\tLoss: 0.838407\n",
      "Train Epoch: 1 [19208/39782 (48%)]\tLoss: 0.840452\n",
      "Train Epoch: 1 [19306/39782 (49%)]\tLoss: 0.842981\n",
      "Train Epoch: 1 [19404/39782 (49%)]\tLoss: 0.848860\n",
      "Train Epoch: 1 [19502/39782 (49%)]\tLoss: 0.841870\n",
      "Train Epoch: 1 [19600/39782 (49%)]\tLoss: 0.839276\n",
      "Train Epoch: 1 [19698/39782 (50%)]\tLoss: 0.839566\n",
      "Train Epoch: 1 [19796/39782 (50%)]\tLoss: 0.846101\n",
      "Train Epoch: 1 [19894/39782 (50%)]\tLoss: 0.846712\n",
      "Train Epoch: 1 [19992/39782 (50%)]\tLoss: 0.840958\n",
      "Train Epoch: 1 [20090/39782 (51%)]\tLoss: 0.847199\n",
      "Train Epoch: 1 [20188/39782 (51%)]\tLoss: 0.839541\n",
      "Train Epoch: 1 [20286/39782 (51%)]\tLoss: 0.842457\n",
      "Train Epoch: 1 [20384/39782 (51%)]\tLoss: 0.839240\n",
      "Train Epoch: 1 [20482/39782 (52%)]\tLoss: 0.844327\n",
      "Train Epoch: 1 [20580/39782 (52%)]\tLoss: 0.841406\n",
      "Train Epoch: 1 [20678/39782 (52%)]\tLoss: 0.841279\n",
      "Train Epoch: 1 [20776/39782 (52%)]\tLoss: 0.840265\n",
      "Train Epoch: 1 [20874/39782 (53%)]\tLoss: 0.847108\n",
      "Train Epoch: 1 [20972/39782 (53%)]\tLoss: 0.844569\n",
      "Train Epoch: 1 [21070/39782 (53%)]\tLoss: 0.842990\n",
      "Train Epoch: 1 [21168/39782 (53%)]\tLoss: 0.846991\n",
      "Train Epoch: 1 [21266/39782 (54%)]\tLoss: 0.842570\n",
      "Train Epoch: 1 [21364/39782 (54%)]\tLoss: 0.841805\n",
      "Train Epoch: 1 [21462/39782 (54%)]\tLoss: 0.842664\n",
      "Train Epoch: 1 [21560/39782 (54%)]\tLoss: 0.841833\n",
      "Train Epoch: 1 [21658/39782 (55%)]\tLoss: 0.844569\n",
      "Train Epoch: 1 [21756/39782 (55%)]\tLoss: 0.845564\n",
      "Train Epoch: 1 [21854/39782 (55%)]\tLoss: 0.843138\n",
      "Train Epoch: 1 [21952/39782 (55%)]\tLoss: 0.841679\n",
      "Train Epoch: 1 [22050/39782 (56%)]\tLoss: 0.840906\n",
      "Train Epoch: 1 [22148/39782 (56%)]\tLoss: 0.835322\n",
      "Train Epoch: 1 [22246/39782 (56%)]\tLoss: 0.848208\n",
      "Train Epoch: 1 [22344/39782 (56%)]\tLoss: 0.840250\n",
      "Train Epoch: 1 [22442/39782 (57%)]\tLoss: 0.841998\n",
      "Train Epoch: 1 [22540/39782 (57%)]\tLoss: 0.845189\n",
      "Train Epoch: 1 [22638/39782 (57%)]\tLoss: 0.836410\n",
      "Train Epoch: 1 [22736/39782 (57%)]\tLoss: 0.845713\n",
      "Train Epoch: 1 [22834/39782 (58%)]\tLoss: 0.836946\n",
      "Train Epoch: 1 [22932/39782 (58%)]\tLoss: 0.834619\n",
      "Train Epoch: 1 [23030/39782 (58%)]\tLoss: 0.835538\n",
      "Train Epoch: 1 [23128/39782 (58%)]\tLoss: 0.840626\n",
      "Train Epoch: 1 [23226/39782 (59%)]\tLoss: 0.846773\n",
      "Train Epoch: 1 [23324/39782 (59%)]\tLoss: 0.846654\n",
      "Train Epoch: 1 [23422/39782 (59%)]\tLoss: 0.848582\n",
      "Train Epoch: 1 [23520/39782 (59%)]\tLoss: 0.836522\n",
      "Train Epoch: 1 [23618/39782 (60%)]\tLoss: 0.845624\n",
      "Train Epoch: 1 [23716/39782 (60%)]\tLoss: 0.840125\n",
      "Train Epoch: 1 [23814/39782 (60%)]\tLoss: 0.842118\n",
      "Train Epoch: 1 [23912/39782 (60%)]\tLoss: 0.841782\n",
      "Train Epoch: 1 [24010/39782 (60%)]\tLoss: 0.845184\n",
      "Train Epoch: 1 [24108/39782 (61%)]\tLoss: 0.832336\n",
      "Train Epoch: 1 [24206/39782 (61%)]\tLoss: 0.846215\n",
      "Train Epoch: 1 [24304/39782 (61%)]\tLoss: 0.842978\n",
      "Train Epoch: 1 [24402/39782 (61%)]\tLoss: 0.840590\n",
      "Train Epoch: 1 [24500/39782 (62%)]\tLoss: 0.839635\n",
      "Train Epoch: 1 [24598/39782 (62%)]\tLoss: 0.839857\n",
      "Train Epoch: 1 [24696/39782 (62%)]\tLoss: 0.847666\n",
      "Train Epoch: 1 [24794/39782 (62%)]\tLoss: 0.852538\n",
      "Train Epoch: 1 [24892/39782 (63%)]\tLoss: 0.843256\n",
      "Train Epoch: 1 [24990/39782 (63%)]\tLoss: 0.839908\n",
      "Train Epoch: 1 [25088/39782 (63%)]\tLoss: 0.847100\n",
      "Train Epoch: 1 [25186/39782 (63%)]\tLoss: 0.839991\n",
      "Train Epoch: 1 [25284/39782 (64%)]\tLoss: 0.841132\n",
      "Train Epoch: 1 [25382/39782 (64%)]\tLoss: 0.836736\n",
      "Train Epoch: 1 [25480/39782 (64%)]\tLoss: 0.837567\n",
      "Train Epoch: 1 [25578/39782 (64%)]\tLoss: 0.844126\n",
      "Train Epoch: 1 [25676/39782 (65%)]\tLoss: 0.838884\n",
      "Train Epoch: 1 [25774/39782 (65%)]\tLoss: 0.840890\n",
      "Train Epoch: 1 [25872/39782 (65%)]\tLoss: 0.838891\n",
      "Train Epoch: 1 [25970/39782 (65%)]\tLoss: 0.841595\n",
      "Train Epoch: 1 [26068/39782 (66%)]\tLoss: 0.835888\n",
      "Train Epoch: 1 [26166/39782 (66%)]\tLoss: 0.843530\n",
      "Train Epoch: 1 [26264/39782 (66%)]\tLoss: 0.840597\n",
      "Train Epoch: 1 [26362/39782 (66%)]\tLoss: 0.845055\n",
      "Train Epoch: 1 [26460/39782 (67%)]\tLoss: 0.844434\n",
      "Train Epoch: 1 [26558/39782 (67%)]\tLoss: 0.845270\n",
      "Train Epoch: 1 [26656/39782 (67%)]\tLoss: 0.841921\n",
      "Train Epoch: 1 [26754/39782 (67%)]\tLoss: 0.842224\n",
      "Train Epoch: 1 [26852/39782 (68%)]\tLoss: 0.848561\n",
      "Train Epoch: 1 [26950/39782 (68%)]\tLoss: 0.840791\n",
      "Train Epoch: 1 [27048/39782 (68%)]\tLoss: 0.843868\n",
      "Train Epoch: 1 [27146/39782 (68%)]\tLoss: 0.846884\n",
      "Train Epoch: 1 [27244/39782 (69%)]\tLoss: 0.839911\n",
      "Train Epoch: 1 [27342/39782 (69%)]\tLoss: 0.839716\n",
      "Train Epoch: 1 [27440/39782 (69%)]\tLoss: 0.840959\n",
      "Train Epoch: 1 [27538/39782 (69%)]\tLoss: 0.836972\n",
      "Train Epoch: 1 [27636/39782 (70%)]\tLoss: 0.847905\n",
      "Train Epoch: 1 [27734/39782 (70%)]\tLoss: 0.842522\n",
      "Train Epoch: 1 [27832/39782 (70%)]\tLoss: 0.848563\n",
      "Train Epoch: 1 [27930/39782 (70%)]\tLoss: 0.846170\n",
      "Train Epoch: 1 [28028/39782 (71%)]\tLoss: 0.845608\n",
      "Train Epoch: 1 [28126/39782 (71%)]\tLoss: 0.841895\n",
      "Train Epoch: 1 [28224/39782 (71%)]\tLoss: 0.840575\n",
      "Train Epoch: 1 [28322/39782 (71%)]\tLoss: 0.842239\n",
      "Train Epoch: 1 [28420/39782 (72%)]\tLoss: 0.845104\n",
      "Train Epoch: 1 [28518/39782 (72%)]\tLoss: 0.836517\n",
      "Train Epoch: 1 [28616/39782 (72%)]\tLoss: 0.838641\n",
      "Train Epoch: 1 [28714/39782 (72%)]\tLoss: 0.847356\n",
      "Train Epoch: 1 [28812/39782 (73%)]\tLoss: 0.835876\n",
      "Train Epoch: 1 [28910/39782 (73%)]\tLoss: 0.842786\n",
      "Train Epoch: 1 [29008/39782 (73%)]\tLoss: 0.843804\n",
      "Train Epoch: 1 [29106/39782 (73%)]\tLoss: 0.842664\n",
      "Train Epoch: 1 [29204/39782 (74%)]\tLoss: 0.845217\n",
      "Train Epoch: 1 [29302/39782 (74%)]\tLoss: 0.843682\n",
      "Train Epoch: 1 [29400/39782 (74%)]\tLoss: 0.836427\n",
      "Train Epoch: 1 [29498/39782 (74%)]\tLoss: 0.840987\n",
      "Train Epoch: 1 [29596/39782 (75%)]\tLoss: 0.835277\n",
      "Train Epoch: 1 [29694/39782 (75%)]\tLoss: 0.836480\n",
      "Train Epoch: 1 [29792/39782 (75%)]\tLoss: 0.843040\n",
      "Train Epoch: 1 [29890/39782 (75%)]\tLoss: 0.844490\n",
      "Train Epoch: 1 [29988/39782 (76%)]\tLoss: 0.838985\n",
      "Train Epoch: 1 [30086/39782 (76%)]\tLoss: 0.841345\n",
      "Train Epoch: 1 [30184/39782 (76%)]\tLoss: 0.840458\n",
      "Train Epoch: 1 [30282/39782 (76%)]\tLoss: 0.836795\n",
      "Train Epoch: 1 [30380/39782 (77%)]\tLoss: 0.841301\n",
      "Train Epoch: 1 [30478/39782 (77%)]\tLoss: 0.842834\n",
      "Train Epoch: 1 [30576/39782 (77%)]\tLoss: 0.835657\n",
      "Train Epoch: 1 [30674/39782 (77%)]\tLoss: 0.841096\n",
      "Train Epoch: 1 [30772/39782 (78%)]\tLoss: 0.840158\n",
      "Train Epoch: 1 [30870/39782 (78%)]\tLoss: 0.838651\n",
      "Train Epoch: 1 [30968/39782 (78%)]\tLoss: 0.842778\n",
      "Train Epoch: 1 [31066/39782 (78%)]\tLoss: 0.844873\n",
      "Train Epoch: 1 [31164/39782 (79%)]\tLoss: 0.843788\n",
      "Train Epoch: 1 [31262/39782 (79%)]\tLoss: 0.841145\n",
      "Train Epoch: 1 [31360/39782 (79%)]\tLoss: 0.842627\n",
      "Train Epoch: 1 [31458/39782 (79%)]\tLoss: 0.836298\n",
      "Train Epoch: 1 [31556/39782 (80%)]\tLoss: 0.842071\n",
      "Train Epoch: 1 [31654/39782 (80%)]\tLoss: 0.845993\n",
      "Train Epoch: 1 [31752/39782 (80%)]\tLoss: 0.843905\n",
      "Train Epoch: 1 [31850/39782 (80%)]\tLoss: 0.838654\n",
      "Train Epoch: 1 [31948/39782 (80%)]\tLoss: 0.841117\n",
      "Train Epoch: 1 [32046/39782 (81%)]\tLoss: 0.841222\n",
      "Train Epoch: 1 [32144/39782 (81%)]\tLoss: 0.841002\n",
      "Train Epoch: 1 [32242/39782 (81%)]\tLoss: 0.848042\n",
      "Train Epoch: 1 [32340/39782 (81%)]\tLoss: 0.841943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32438/39782 (82%)]\tLoss: 0.839898\n",
      "Train Epoch: 1 [32536/39782 (82%)]\tLoss: 0.841684\n",
      "Train Epoch: 1 [32634/39782 (82%)]\tLoss: 0.844484\n",
      "Train Epoch: 1 [32732/39782 (82%)]\tLoss: 0.843640\n",
      "Train Epoch: 1 [32830/39782 (83%)]\tLoss: 0.842312\n",
      "Train Epoch: 1 [32928/39782 (83%)]\tLoss: 0.838732\n",
      "Train Epoch: 1 [33026/39782 (83%)]\tLoss: 0.839257\n",
      "Train Epoch: 1 [33124/39782 (83%)]\tLoss: 0.841493\n",
      "Train Epoch: 1 [33222/39782 (84%)]\tLoss: 0.843765\n",
      "Train Epoch: 1 [33320/39782 (84%)]\tLoss: 0.839777\n",
      "Train Epoch: 1 [33418/39782 (84%)]\tLoss: 0.842296\n",
      "Train Epoch: 1 [33516/39782 (84%)]\tLoss: 0.842929\n",
      "Train Epoch: 1 [33614/39782 (85%)]\tLoss: 0.839241\n",
      "Train Epoch: 1 [33712/39782 (85%)]\tLoss: 0.844337\n",
      "Train Epoch: 1 [33810/39782 (85%)]\tLoss: 0.847120\n",
      "Train Epoch: 1 [33908/39782 (85%)]\tLoss: 0.837522\n",
      "Train Epoch: 1 [34006/39782 (86%)]\tLoss: 0.843171\n",
      "Train Epoch: 1 [34104/39782 (86%)]\tLoss: 0.838101\n",
      "Train Epoch: 1 [34202/39782 (86%)]\tLoss: 0.846140\n",
      "Train Epoch: 1 [34300/39782 (86%)]\tLoss: 0.843359\n",
      "Train Epoch: 1 [34398/39782 (87%)]\tLoss: 0.839077\n",
      "Train Epoch: 1 [34496/39782 (87%)]\tLoss: 0.840274\n",
      "Train Epoch: 1 [34594/39782 (87%)]\tLoss: 0.841971\n",
      "Train Epoch: 1 [34692/39782 (87%)]\tLoss: 0.843160\n",
      "Train Epoch: 1 [34790/39782 (88%)]\tLoss: 0.848390\n",
      "Train Epoch: 1 [34888/39782 (88%)]\tLoss: 0.844704\n",
      "Train Epoch: 1 [34986/39782 (88%)]\tLoss: 0.840991\n",
      "Train Epoch: 1 [35084/39782 (88%)]\tLoss: 0.838194\n",
      "Train Epoch: 1 [35182/39782 (89%)]\tLoss: 0.834958\n",
      "Train Epoch: 1 [35280/39782 (89%)]\tLoss: 0.839749\n",
      "Train Epoch: 1 [35378/39782 (89%)]\tLoss: 0.844683\n",
      "Train Epoch: 1 [35476/39782 (89%)]\tLoss: 0.845879\n",
      "Train Epoch: 1 [35574/39782 (90%)]\tLoss: 0.836642\n",
      "Train Epoch: 1 [35672/39782 (90%)]\tLoss: 0.839620\n",
      "Train Epoch: 1 [35770/39782 (90%)]\tLoss: 0.840512\n",
      "Train Epoch: 1 [35868/39782 (90%)]\tLoss: 0.844190\n",
      "Train Epoch: 1 [35966/39782 (91%)]\tLoss: 0.845784\n",
      "Train Epoch: 1 [36064/39782 (91%)]\tLoss: 0.843236\n",
      "Train Epoch: 1 [36162/39782 (91%)]\tLoss: 0.836735\n",
      "Train Epoch: 1 [36260/39782 (91%)]\tLoss: 0.841478\n",
      "Train Epoch: 1 [36358/39782 (92%)]\tLoss: 0.836368\n",
      "Train Epoch: 1 [36456/39782 (92%)]\tLoss: 0.838562\n",
      "Train Epoch: 1 [36554/39782 (92%)]\tLoss: 0.844070\n",
      "Train Epoch: 1 [36652/39782 (92%)]\tLoss: 0.845830\n",
      "Train Epoch: 1 [36750/39782 (93%)]\tLoss: 0.848187\n",
      "Train Epoch: 1 [36848/39782 (93%)]\tLoss: 0.840209\n",
      "Train Epoch: 1 [36946/39782 (93%)]\tLoss: 0.842973\n",
      "Train Epoch: 1 [37044/39782 (93%)]\tLoss: 0.836352\n",
      "Train Epoch: 1 [37142/39782 (94%)]\tLoss: 0.838399\n",
      "Train Epoch: 1 [37240/39782 (94%)]\tLoss: 0.839262\n",
      "Train Epoch: 1 [37338/39782 (94%)]\tLoss: 0.838748\n",
      "Train Epoch: 1 [37436/39782 (94%)]\tLoss: 0.840483\n",
      "Train Epoch: 1 [37534/39782 (95%)]\tLoss: 0.840656\n",
      "Train Epoch: 1 [37632/39782 (95%)]\tLoss: 0.840646\n",
      "Train Epoch: 1 [37730/39782 (95%)]\tLoss: 0.838443\n",
      "Train Epoch: 1 [37828/39782 (95%)]\tLoss: 0.842459\n",
      "Train Epoch: 1 [37926/39782 (96%)]\tLoss: 0.842436\n",
      "Train Epoch: 1 [38024/39782 (96%)]\tLoss: 0.836600\n",
      "Train Epoch: 1 [38122/39782 (96%)]\tLoss: 0.841127\n",
      "Train Epoch: 1 [38220/39782 (96%)]\tLoss: 0.845331\n",
      "Train Epoch: 1 [38318/39782 (97%)]\tLoss: 0.839678\n",
      "Train Epoch: 1 [38416/39782 (97%)]\tLoss: 0.840955\n",
      "Train Epoch: 1 [38514/39782 (97%)]\tLoss: 0.842846\n",
      "Train Epoch: 1 [38612/39782 (97%)]\tLoss: 0.838411\n",
      "Train Epoch: 1 [38710/39782 (98%)]\tLoss: 0.839962\n",
      "Train Epoch: 1 [38808/39782 (98%)]\tLoss: 0.838159\n",
      "Train Epoch: 1 [38906/39782 (98%)]\tLoss: 0.837511\n",
      "Train Epoch: 1 [39004/39782 (98%)]\tLoss: 0.844724\n",
      "Train Epoch: 1 [39102/39782 (99%)]\tLoss: 0.842303\n",
      "Train Epoch: 1 [39200/39782 (99%)]\tLoss: 0.834882\n",
      "Train Epoch: 1 [39298/39782 (99%)]\tLoss: 0.850579\n",
      "Train Epoch: 1 [39396/39782 (99%)]\tLoss: 0.839960\n",
      "Train Epoch: 1 [39494/39782 (100%)]\tLoss: 0.845012\n",
      "Train Epoch: 1 [39592/39782 (100%)]\tLoss: 0.847624\n",
      "====> Epoch: 1 Average Loss: 0.8492\n",
      "====> Test set Loss: 0.8486\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model,'/media/EXTHD/niciData/models/YamahaPianoComp2002_5Epochs_LSTM_noTW.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.set_printoptions(precision=2, suppress=True, threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "playSeq = 0\n",
    "pathToSampleSeq = \"/Volumes/EXT/DATASETS/WikifoniaServer/test/Charlie-Tobias,-Nat-Simon---No-Can-Do.mid\"\n",
    "if(model.train()):\n",
    "    model.eval()\n",
    "\n",
    "###PREDICT 8th SEQUENCE\n",
    "with torch.no_grad():\n",
    "    \n",
    "    sampleNp1 = getSlicedPianorollMatrixNp(pathToSampleSeq)\n",
    "    sampleNp1 = deleteZeroMatrices(sampleNp1)\n",
    "    sample = np.expand_dims(sampleNp1[0,:,36:-32],axis=0)\n",
    "    print(sample.shape)\n",
    "    for i, sampleNp in enumerate(sampleNp1[playSeq:playSeq+6]):\n",
    "        print(sampleNp.shape)\n",
    "        if(np.any(sampleNp)):\n",
    "            sampleNp = sampleNp[:,36:-32]\n",
    "            sampleNp = np.expand_dims(sampleNp,axis=0)\n",
    "            sample = np.concatenate((sample,sampleNp),axis=0)\n",
    "    samplePlay = sample[0,:,:]\n",
    "    for s in sample[1:]:\n",
    "        samplePlay = np.concatenate((samplePlay,s),axis=0)\n",
    "    samplePlay = addCuttedOctaves(samplePlay)\n",
    "    print(samplePlay.shape)\n",
    "    sample = torch.from_numpy(sample).float().to(device)\n",
    "    sample = torch.unsqueeze(sample,1)\n",
    "    print(sample.size())\n",
    "    _,_, pred = model(sample)\n",
    "    #reconstruction = recon.squeeze(0).squeeze(0).cpu().numpy()\n",
    "    prediction = pred.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "    #print(sampleNp[:,:])\n",
    "    #print(prediction[:,:])\n",
    "    #print(np.sum(sampleNp.numpy(), axis=1))\n",
    "\n",
    "    #NORMALIZE PREDICTIONS\n",
    "    #reconstruction /= np.abs(np.max(reconstruction))\n",
    "    prediction /= np.abs(np.max(prediction))\n",
    "    #print(prediction)\n",
    "\n",
    "    #CHECK MIDI ACTIVATIONS IN PREDICTION TO INCLUDE RESTS\n",
    "    #reconstruction[reconstruction < 0.3] = 0\n",
    "    prediction[prediction < 0.65] = 0\n",
    "\n",
    "\n",
    "\n",
    "    ###MONOPHONIC OUTPUT MATRIX POLOYPHONIC POSSIBLE WITH ACTIVATION THRESHOLD###\n",
    "    #score = music21.converter.parse('WikifoniaServer/samples/The-Doors---Don\\'t-you-love-her-Madly?.mid')\n",
    "    #score.show()\n",
    "\n",
    "    samplePlay = debinarizeMidi(samplePlay, prediction=False)\n",
    "    samplePlay = addCuttedOctaves(samplePlay)\n",
    "    #reconstruction = debinarizeMidi(reconstruction, prediction=True)\n",
    "    #reconstruction = addCuttedOctaves(reconstruction)\n",
    "    prediction = debinarizeMidi(prediction, prediction=True)\n",
    "    prediction = addCuttedOctaves(prediction)\n",
    "\n",
    "    #print(np.argmax(samplePlay, axis=1))\n",
    "    #print('')\n",
    "    #print(np.argmax(prediction, axis=1))\n",
    "    print(\"INPUT\")\n",
    "    print(samplePlay.shape)\n",
    "    pianorollMatrixToTempMidi(samplePlay, show=True,showPlayer=True,autoplay=False)\n",
    "    #print(\"RECONSTRUCTION\")\n",
    "    #pianorollMatrixToTempMidi(reconstruction, show=True,\n",
    "    #                            showPlayer=True,autoplay=True, prediction=True)\n",
    "    print(\"PREDICTION\")\n",
    "    pianorollMatrixToTempMidi(prediction, prediction=False, \n",
    "                              show=True,showPlayer=True,autoplay=True)        \n",
    "    print(\"\\n\\n\")\n",
    "            \n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
